{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Processing with HMMs and CRFs\n",
    "\n",
    "**The goal of this practical is to study sequence models in NLP.**\n",
    "\n",
    "We will work on Part-Of-Speech (POS) and optionally on chunking (gathering different groups in sentences). The datasets are from [CONLL 2000](https://www.clips.uantwerpen.be/conll2000/chunking/): \n",
    "- **Small corpus:** chtrain/chtest to understand the tools and models \n",
    "- **Larger corpus:** train/test to collect reliable experimental results\n",
    "\n",
    "\n",
    "# 1) HMMS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading POS/Chunking data\n",
    "def load(filename):\n",
    "    listeDoc = list()\n",
    "    with open(filename, \"r\") as f:\n",
    "        doc = list()\n",
    "        for ligne in f:\n",
    "            if len(ligne) < 2: # fin de doc\n",
    "                listeDoc.append(doc)\n",
    "                doc = list()\n",
    "                continue\n",
    "            mots = ligne.replace(\"\\n\",\"\").split(\" \")\n",
    "            doc.append((mots[0],mots[1])) # Change mots[1] -> mots[2] for chuncking\n",
    "    return listeDoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "823  docs read\n",
      "77  docs (T) read\n"
     ]
    }
   ],
   "source": [
    "# =============== loding ============\n",
    "# small corpus => ideal for first tests\n",
    "filename = \"./datasets/conll2000/chtrain.txt\" \n",
    "filenameT = \"./datasets//conll2000/chtest.txt\" \n",
    "\n",
    "# Larger corpus => To valide perf.\n",
    "# filename = \"ressources/conll2000/train.txt\" \n",
    "# filenameT = \"ressources/conll2000/test.txt\" \n",
    "\n",
    "alldocs = load(filename)\n",
    "alldocsT = load(filenameT)\n",
    "\n",
    "print(len(alldocs),\" docs read\")\n",
    "print(len(alldocsT),\" docs (T) read\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Rockwell', 'NNP'), ('International', 'NNP'), ('Corp.', 'NNP'), (\"'s\", 'POS'), ('Tulsa', 'NNP'), ('unit', 'NN'), ('said', 'VBD'), ('it', 'PRP'), ('signed', 'VBD'), ('a', 'DT'), ('tentative', 'JJ'), ('agreement', 'NN'), ('extending', 'VBG'), ('its', 'PRP$'), ('contract', 'NN'), ('with', 'IN'), ('Boeing', 'NNP'), ('Co.', 'NNP'), ('to', 'TO'), ('provide', 'VB'), ('structural', 'JJ'), ('parts', 'NNS'), ('for', 'IN'), ('Boeing', 'NNP'), (\"'s\", 'POS'), ('747', 'CD'), ('jetliners', 'NNS'), ('.', '.')]\n",
      "[('Confidence', 'NN'), ('in', 'IN'), ('the', 'DT'), ('pound', 'NN'), ('is', 'VBZ'), ('widely', 'RB'), ('expected', 'VBN'), ('to', 'TO'), ('take', 'VB'), ('another', 'DT'), ('sharp', 'JJ'), ('dive', 'NN'), ('if', 'IN'), ('trade', 'NN'), ('figures', 'NNS'), ('for', 'IN'), ('September', 'NNP'), (',', ','), ('due', 'JJ'), ('for', 'IN'), ('release', 'NN'), ('tomorrow', 'NN'), (',', ','), ('fail', 'VB'), ('to', 'TO'), ('show', 'VB'), ('a', 'DT'), ('substantial', 'JJ'), ('improvement', 'NN'), ('from', 'IN'), ('July', 'NNP'), ('and', 'CC'), ('August', 'NNP'), (\"'s\", 'POS'), ('near-record', 'JJ'), ('deficits', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(alldocs[0])\n",
    "print(alldocsT[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a baseline POS model (without sequence)\n",
    "\n",
    "We will build a simple dictionary ```word => PoS label``` without taking into account any sequence information. We will compare the sequence models to this baseline.\n",
    "\n",
    "The dataset is a list a tuples with ```(word, POS)```. **Build a simple dictionary mapping each word to its PoS tag in the train set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary building \n",
    "dico = dict()\n",
    "\n",
    "for doc in alldocs:\n",
    "    for mots, pos in doc:\n",
    "        dico[mots] = pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: on the test set, there are unknown words...**. We will use the following simple strategy: \n",
    "```\n",
    "# remplace\n",
    "dico[cle] # crashing with an unknown key \n",
    "# by \n",
    "dico.get(cle, DefaultValue)\n",
    "```\n",
    "From a linguistic point of view, we can choose the default value as the majority PoS class, producing a stronger baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1433 good prediction over 1896\n"
     ]
    }
   ],
   "source": [
    "# Evaluate test performances\n",
    "\n",
    "pred = []\n",
    "\n",
    "for i in range(len(alldocsT)):\n",
    "    p = []\n",
    "    for mot, cle in alldocsT[i]:\n",
    "        p.append((mot, dico.get(mot, None)))\n",
    "    pred.append(p)\n",
    "\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for i in range(len(alldocsT)):\n",
    "    for j in range(len(alldocsT[i])):\n",
    "        pos_true = alldocsT[i][j][1]\n",
    "        pos_pred = pred[i][j][1]\n",
    "\n",
    "        if pos_true == pos_pred:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "print(f\"{correct} good prediction over {total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check: 1433 good predictions in test over 1896\n",
    "\n",
    "(1527 with 'NN' as default PoS value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HMMs\n",
    "\n",
    "Here is a code for training HMM parameters and running decoding using the Viterbi algorithm. You should apply it to our PoS task. **N.B.: you should undersand the ```eps``` parmaters**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allx: list of observation sequences\n",
    "# allq: list os state sequences\n",
    "# N: nb states\n",
    "# K: nb observations\n",
    "\n",
    "\n",
    "def learnHMM(allx, allq, N, K, initTo1=True):\n",
    "    if initTo1:\n",
    "        eps = 1e-1  # You can play with this regularization parameter\n",
    "        A = np.ones((N, N)) * eps\n",
    "        B = np.ones((N, K)) * eps\n",
    "        Pi = np.ones(N) * eps\n",
    "    else:\n",
    "        A = np.zeros((N, N))\n",
    "        B = np.zeros((N, K))\n",
    "        Pi = np.zeros(N)\n",
    "    for x, q in zip(allx, allq):\n",
    "        Pi[int(q[0])] += 1\n",
    "        for i in range(len(q) - 1):\n",
    "            A[int(q[i]), int(q[i + 1])] += 1\n",
    "            B[int(q[i]), int(x[i])] += 1\n",
    "        B[int(q[-1]), int(x[-1])] += 1  # last transition\n",
    "    A = A / np.maximum(A.sum(1).reshape(N, 1), 1)  # normalisation\n",
    "    B = B / np.maximum(B.sum(1).reshape(N, 1), 1)  # normalisation\n",
    "    Pi = Pi / Pi.sum()\n",
    "    return Pi, A, B\n",
    "\n",
    "\n",
    "def viterbi(x, Pi, A, B):\n",
    "    T = len(x)\n",
    "    N = len(Pi)\n",
    "    logA = np.log(A)\n",
    "    logB = np.log(B)\n",
    "    logdelta = np.zeros((N, T))\n",
    "    psi = np.zeros((N, T), dtype=int)\n",
    "    S = np.zeros(T)\n",
    "    logdelta[:, 0] = np.log(Pi) + logB[:, int(x[0])]\n",
    "    # forward\n",
    "    for t in range(1, T):\n",
    "        logdelta[:, t] = (logdelta[:, t - 1].reshape(N, 1) + logA).max(0) + logB[\n",
    "            :, int(x[t])\n",
    "        ]\n",
    "        psi[:, t] = (logdelta[:, t - 1].reshape(N, 1) + logA).argmax(0)\n",
    "    # backward\n",
    "    logp = logdelta[:, -1].max()\n",
    "    S[T - 1] = logdelta[:, -1].argmax()\n",
    "    for i in range(2, T + 1):\n",
    "        S[int(T - i)] = psi[int(S[int(T - i + 1)]), int(T - i + 1)]\n",
    "    return S, logp  # , delta, psi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data encoding\n",
    "\n",
    "We will map each word to an index for traing the HMM (see code below):\n",
    "```\n",
    " The cat is in the garden => 1 2 3 4 1 5\n",
    "```\n",
    "We have to understand the dictionary functionning to retrieve the words corresponding to indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4570 42  in the dictionary\n"
     ]
    }
   ],
   "source": [
    "# alldocs etant issu du chargement des données\n",
    "# la mise en forme des données est fournie ici\n",
    "# afin de produire des analyses qualitative, vous devez malgré tout comprendre le fonctionnement des dictionnaires\n",
    "\n",
    "buf = [[m for m,pos in d ] for d in alldocs]\n",
    "mots = []\n",
    "[mots.extend(b) for b in buf]\n",
    "mots = np.unique(np.array(mots))\n",
    "nMots = len(mots)+1 # mot inconnu\n",
    "\n",
    "mots2ind = dict(zip(mots,range(len(mots))))\n",
    "mots2ind[\"UUUUUUUU\"] = len(mots)\n",
    "\n",
    "buf2 = [[pos for m,pos in d ] for d in alldocs]\n",
    "cles = []\n",
    "[cles.extend(b) for b in buf2]\n",
    "cles = np.unique(np.array(cles))\n",
    "cles2ind = dict(zip(cles,range(len(cles))))\n",
    "\n",
    "nCles = len(cles)\n",
    "\n",
    "print(nMots,nCles,\" in the dictionary\")\n",
    "\n",
    "# mise en forme des données\n",
    "allx  = [[mots2ind[m] for m,pos in d] for d in alldocs]\n",
    "allxT = [[mots2ind.setdefault(m,len(mots)) for m,pos in d] for d in alldocsT]\n",
    "\n",
    "allq  = [[cles2ind[pos] for m,pos in d] for d in alldocs]\n",
    "allqT = [[cles2ind.setdefault(pos,len(cles)) for m,pos in d] for d in alldocsT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1140, 814, 563, 11, 1294, 4393, 3855, 2854, 3992, 1362, 4242, 1452, 2395, 2855, 1990, 4529, 446, 525, 4299, 3595, 4148, 3368, 2499, 446, 11, 283, 2861, 20]\n",
      "[18, 18, 18, 22, 18, 17, 32, 23, 32, 9, 13, 17, 33, 24, 17, 12, 18, 18, 29, 31, 13, 20, 12, 18, 22, 8, 20, 5]\n"
     ]
    }
   ],
   "source": [
    "# First doc:\n",
    "print(allx[0])\n",
    "print(allq[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You turn: apply HMMs to those data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HMM training\n",
    "Pi, A, B = learnHMM(allx, allq, nCles, nMots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1565 good prediction over 1896\n"
     ]
    }
   ],
   "source": [
    "# HMM decoding and performances evaluation\n",
    "allqT_predict = []\n",
    "for x in allxT:\n",
    "    S, _ = viterbi(x, Pi, A, B)\n",
    "    allqT_predict.append(S)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for i in range(len(allqT)):\n",
    "    for j in range(len(allqT[i])):\n",
    "        pos_true = allqT[i][j]\n",
    "        pos_pred = int(allqT_predict[i][j])\n",
    "        \n",
    "        if pos_true == pos_pred:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "print(f\"{correct} good prediction over {total}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check : 1564 in test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualitative Analyis:\n",
    "\n",
    "- With imshow on the parameters (ou d'un argsort), show what are the probable transition between labels.\n",
    "- Visualize the confusion matrices to understand what is challenging in this task\n",
    "- Find out examples that are corrected by Viterbi decoding\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Conditional Random Fields (CRF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CRF are disciminative models** representing the conditional distribution $ P( \\mathbf{y} | \\mathbf{x} , \\mathbf{w}) $:\n",
    "\n",
    "$$\n",
    "P(\\mathbf{y}\\mid \\mathbf{x},\\mathbf{w})\n",
    "= \\frac{e^{\\mathbf{w}^\\top \\psi(\\mathbf{x},\\mathbf{y})}}\n",
    "{\\sum\\limits_{y' \\in \\mathcal{Y}} e^{\\mathbf{w}^\\top \\psi(\\mathbf{x},\\mathbf{y}')}}\n",
    "$$       \n",
    "**In 'linear-chain' CRFs**, the feature functions include **unary terms $u_k$** ($\\sim$ $\\mathbf{B}$ matrix in HMMs) and **pairwise terms $p_k$** ($\\sim$ $\\mathbf{A}$ matrix in HMMs):\n",
    "\n",
    "$$ \\mathbf{w}^T \\psi(\\mathbf{x},\\mathbf{y}) = \\sum\\limits_{t=1}^T \\sum_{k=1}^K F_k(y_{t-1}, y_t, \\mathbf{x})  =   \\sum\\limits_{t=1}^T \\sum_{k=1}^K \\left[ u_k(y_t, \\mathbf{x}) + p_k(y_{t-1}, y_t, \\mathbf{x}) \\right]$$\n",
    "\n",
    "[<img src=\"https://thome.isir.upmc.fr/classes/RITAL/crf-obs2.png\" width=\"800\" >](https://thome.isir.upmc.fr/classes/RITAL/crf-obs2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can directly use resources from nltk: \n",
    "- [CRFTagger](https://tedboy.github.io/nlps/generated/generated/nltk.CRFTagger.html)\n",
    "- [PerceptronTagger](https://www.nltk.org/_modules/nltk/tag/perceptron.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldocsTword = []\n",
    "\n",
    "for i in range(len(alldocsT)):\n",
    "    p = []\n",
    "    for mot, cle in alldocsT[i]:\n",
    "        p.append(mot)\n",
    "    alldocsTword.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1720 good prediction over 1896\n"
     ]
    }
   ],
   "source": [
    "# !pip install python-crfsuite\n",
    "from nltk.tag.crf import CRFTagger\n",
    "\n",
    "tagger = CRFTagger()\n",
    "tagger.train(alldocs, \"model.crf.tagger\")  # training\n",
    "\n",
    "# tagger.tag_sents(alldocsTword)\n",
    "accuracy = tagger.accuracy(alldocsT)\n",
    "\n",
    "print(f\"{int(total*accuracy)} good prediction over {total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check: 1720 bonnes réponses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and evaluating the model, as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1736 good prediction over 1896\n"
     ]
    }
   ],
   "source": [
    "# perceptron\n",
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "\n",
    "tagger = PerceptronTagger(load=False)\n",
    "tagger.train(alldocs)\n",
    "accuracy = tagger.accuracy(alldocsT)\n",
    "\n",
    "print(f\"{int(total*accuracy)} good prediction over {total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Going further\n",
    "\n",
    "- We test the application for PoS, we can run similar experiments for chunking (see parsing indication, very simple to load data)\n",
    "- Run  experiement on the larger dataset. This dataset is still largely used in research. This work can thus be included in your resume :)\n",
    "- Work will be purshed with word embeddings (next practical), and for [NER](https://www.clips.uantwerpen.be/conll2003/ner/) with RNNs (X. Tannier)\n",
    "- [State-of-the-art resources](https://github.com/stanfordnlp/stanza/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
